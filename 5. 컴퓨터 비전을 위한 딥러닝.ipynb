{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 합성곱 신경망(컨브넷) 소개\n",
    "\n",
    "간단한 컨브넷 예제   \n",
    "기본적인 컨브넷이어도 완전 연결된 모델의 성능을 훨씬 앞지를 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 합성곱 연산\n",
    "\n",
    "Dense 층: 입력 특성 공간에 있는 전역 패턴 학습   \n",
    "합성곱 층: 지역 패턴 학습 (이미지일 때 작은 2D 윈도우로 입력에서 패턴을 찾음)\n",
    "\n",
    "- 학습된 패턴은 평행 이동 불변성을 가짐   \n",
    "        완전 연결 네트워크는 새로운 위치에 나타난 것은 새로운 패턴으로 학습해야 함\n",
    "        컨브넷은 이미지 효율적으로 처리하게 만들어 줌 (적은 수의 훈련 샘플을 사용해서 일반화 능력 가진 표현을 학습 가능함)\n",
    "- 컨브넷은 패턴의 공간적 계층 구조를 학습할 수 있음   \n",
    "        첫 번째 합성곱이 에지 같은 작은 지역 패턴 학습, 두 번째 합성곱 층은 첫 번째 층의 특성으로 구성된 더 큰 패턴 학습   \n",
    "        -> 매우 복잡하고 추상적인 시각적 개념 효과적으로 학습 가능함\n",
    "\n",
    "합성곱 연산은 특성 맵(3D 텐서)에 적용됨   \n",
    "2개의 공간축(높이, 너비), 깊이(채널)축으로 구성됨  (RGB 이미지는 3개의 컬러 채널 -> 축의 차원이 3)   \n",
    "입력 특성 맵에서 작은 패치들 추출, 모든 패치에 같은 변환 적용해 출력 특성 맵을 만듬\n",
    "\n",
    "출력 텐서(높이, 너비 가진 3D 텐서)의 깊이는 층의 매개변수로 결정됨 -> 깊이 축의 채널은 **필터**를 의미함\n",
    "\n",
    "(28, 28, 1) 크기의 특성 맵 입력 -> (26, 26, 32) 크기의 특성 맵 출력   \n",
    "여기서 입력에 대한 32개의 필터를 적용한 것   \n",
    "32개의 출력 채널 각각은 26x26 크기의 배열 값을 가짐 (입력에 대한 필터의 **응답 맵**   \n",
    "\n",
    "\n",
    "합성곱 정의에 필요한 2개의 파라미터\n",
    "- 입력으로부터 뽑아낼 패치의 크기\n",
    "        보통 3x3, 5x5 크기 사용\n",
    "- 특성 맵의 출력 깊이\n",
    "        합성곱으로 계산할 필터 수\n",
    "        \n",
    "```\n",
    "Conv2D(output_depth, (window_height, window_width))\n",
    "```\n",
    "\n",
    "합성곱은 3D 입력 특성 맵 위를 3x3 또는 5x5 크기의 윈도우가 슬라이딩 하면서 모든 위치에서 3D 특성 패치 추출하는 방식으로 작동함   \n",
    "3D 패치는 (output_depth, ) 크기의 1D 벡터로 변환됨 (합성곱 커널 통해 변환)   \n",
    "변환된 모든 벡터는 (height, width, output_depth) 크기의 3D 특성 맵으로 재구성됨   \n",
    "출력 특성 맵의 공간상 위치는 입력 특성 맵의 같은 위치에 대응됨\n",
    "\n",
    "출력 높이와 너비는 입력 높이와 너비와는 다를 수 있음\n",
    "- 경계 문제\n",
    "        입력과 동일한 높이, 너비 가진 출력 특성 맵 얻고 싶으면 패딩 사용할 수 있음\n",
    "        Conv2D 층에서 padding 매개변수로 설정 가능 (기본값은 valid(패딩 사용 x))\n",
    "- 스트라이드 사용 여부에 따라 다름\n",
    "        스트라이드: 두 번의 연속적인 윈도우 사이의 거리\n",
    "        기본값은 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 최대 풀링 연산\n",
    "\n",
    "최대 풀링의 역할: 강제적으로 특성 맵 다운샘플링 하는 것\n",
    "\n",
    "입력 특성 맵에서 윈도우에 맞는 패치 추출하고 각 채널별 최댓값 출력함 (최댓값 추출 연산 사용)   \n",
    "보통 2x2 윈도우와 스트라이드 2를 사용해 특성 맵을 절반 크기로 다운샘플링함\n",
    "\n",
    "다운샘플링하는 이유: 처리할 특성 맵의 가중치 개수 줄이기 위해서   \n",
    "연속적인 합성곱 층이 점점 커진 윈도우 통해 바라보도록 만들어 필터의 공간적 계층 구조 구성함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 소규모 데이터셋에서 밑바닥부터 컨브넷 훈련하기\n",
    "\n",
    "#### 5.2.1 작은 데이터셋 문제에서 딥러닝의 타당성\n",
    "\n",
    "딥러닝 모델은 조금씩 변경해 다른 문제에 재사용 가능함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 데이터 내려받기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 고양이 이미지 전체 개수: 1000\n",
      "훈련용 강아지 이미지 전체 개수: 1000\n",
      "검증용 강아지 이미지 전체 개수: 500\n",
      "검증용 강아지 이미지 전체 개수: 500\n",
      "테스트용 강아지 이미지 전체 개수: 500\n",
      "테스트용 강아지 이미지 전체 개수: 500\n"
     ]
    }
   ],
   "source": [
    "#훈련, 검증, 테스트 폴더로 이미지 복사하기\n",
    "\n",
    "import os, shutil\n",
    "\n",
    "original_dataset_dir = './datasets/cats_and_dogs/train' #원본 데이터셋 압축 해제한 디렉터리 경로\n",
    "\n",
    "base_dir= './datasets/cats_and_dogs_small' #소규모 데이터셋 저장할 디렉터리\n",
    "#os.mkdir(base_dir)\n",
    "\n",
    "#훈련, 검증, 테스트 분할 위한 디렉터리\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "#os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "#os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "#os.mkdir(test_dir)\n",
    "\n",
    "#훈련용 고양이, 강아지 사진 디렉터리\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "#os.mkdir(train_cats_dir)\n",
    "\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "os.mkdir(train_dogs_dir)\n",
    "\n",
    "#검증용 고양이, 강아지 사진 디렉터리\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "os.mkdir(validation_cats_dir)\n",
    "\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "os.mkdir(validation_dogs_dir)\n",
    "\n",
    "#테스트용 고양이, 강아지 사진 디렉터리\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "os.mkdir(test_cats_dir)\n",
    "\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "os.mkdir(test_dogs_dir)\n",
    "\n",
    "#처음 1000개의 고양이 이미지를 train_cats_dir에 복사\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "#다음 500개의 고양이 이미지를 validation_cats_dir에 복사    \n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000,1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "#다음 500개의 고양이 이미지를 test_cats_dir에 복사\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500,2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "#처음 1000개의 강아지 이미지를 train_cats_dir에 복사\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "#다음 500개의 강아지 이미지를 validation_cats_dir에 복사    \n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000,1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "#다음 500개의 강아지 이미지를 test_cats_dir에 복사\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500,2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "        \n",
    "        \n",
    "#잘 됐는지 사진 개수 카운트로 확인\n",
    "print('훈련용 고양이 이미지 전체 개수:', len(os.listdir(train_cats_dir)))\n",
    "print('훈련용 강아지 이미지 전체 개수:', len(os.listdir(train_dogs_dir)))\n",
    "\n",
    "print('검증용 강아지 이미지 전체 개수:', len(os.listdir(validation_cats_dir)))\n",
    "print('검증용 강아지 이미지 전체 개수:', len(os.listdir(validation_dogs_dir)))\n",
    "\n",
    "print('테스트용 강아지 이미지 전체 개수:', len(os.listdir(test_cats_dir)))\n",
    "print('테스트용 강아지 이미지 전체 개수:', len(os.listdir(test_dogs_dir)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 네트워크 구성하기\n",
    "\n",
    "Conv2D(relu 활성화 함수 사용), MaxPooling2D 층을 번갈아 쌓은 컨브넷 만들기   \n",
    "150x150 크기의 입력으로 시작해서 Flatten 층 이전에 7x7 크기의 특성 맵으로 줄어듬\n",
    "\n",
    "특성 맵의 깊이는 네트워크에서 점진적으로 증가(32 -> 128), 크기는 감소(150x150 -> 7x7)\n",
    "\n",
    "이진 분류 문제라서 네트워크는 하나의 유닛(크기가 1인 Dense 층), sigmoid 활성화 함수로 끝남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#강아지 vs 고양이 분류를 위한 소규모 컨브넷 만들기\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 훈련 설정\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4 데이터 전처리\n",
    "\n",
    "데이터가 JPEG 파일로 되어있어서 네트워크에 주입하려면 과정이 필요함\n",
    "1. 사진 파일을 읽음\n",
    "2. JPEG 콘텐츠를 RGB 픽셀 값으로 디코딩\n",
    "3. 부동 소수 타입의 텐서로 변환\n",
    "4. 픽셀 값(0~255)의 스케일을 [0,1] 사이로 조정\n",
    "\n",
    "keras.preprocessing.image에 이미지 처리 위한 헬퍼 도구들 있음   \n",
    "ImageDataGenerator 클래스는 디스크에 있는 이미지 파일을 전처리된 배치 텐서로 자동으로 바꿔 주는 파이썬 제너레이터를 만들어 줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#ImageDataGenerator 사용해 디렉터리에서 이미지 읽기\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#모든 이미지를 1/255로 스케일 조정\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                   target_size=(150,150), #모든 이미지를 150x150 크기로 바꿈\n",
    "                                                   batch_size=20,\n",
    "                                                   class_mode='binary') #binary_crossentropy 손실 사용해서 이진 레이블 필요함\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir,\n",
    "                                                       target_size=(150,150),\n",
    "                                                       batch_size=20,\n",
    "                                                       class_mode='binary')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-084ed1e6f3dc>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.6907 - acc: 0.5320 - val_loss: 0.6850 - val_acc: 0.5330\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.6606 - acc: 0.6130 - val_loss: 0.6668 - val_acc: 0.5940\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.6274 - acc: 0.6590 - val_loss: 0.6298 - val_acc: 0.6690\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.5876 - acc: 0.7080 - val_loss: 0.6372 - val_acc: 0.6390\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.5419 - acc: 0.7310 - val_loss: 0.5941 - val_acc: 0.6850\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.5109 - acc: 0.7500 - val_loss: 0.6250 - val_acc: 0.6620\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.4822 - acc: 0.7630 - val_loss: 0.6009 - val_acc: 0.6710\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 0.4428 - acc: 0.7940 - val_loss: 0.6583 - val_acc: 0.6690\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.4251 - acc: 0.7990 - val_loss: 0.6400 - val_acc: 0.6760\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.3928 - acc: 0.8250 - val_loss: 0.5478 - val_acc: 0.7210\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.3672 - acc: 0.8390 - val_loss: 0.5696 - val_acc: 0.7140\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.3447 - acc: 0.8460 - val_loss: 0.6015 - val_acc: 0.7160\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.3174 - acc: 0.8665 - val_loss: 0.6288 - val_acc: 0.7290\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 45s 448ms/step - loss: 0.2907 - acc: 0.8800 - val_loss: 0.7770 - val_acc: 0.6990\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.2695 - acc: 0.8890 - val_loss: 0.5939 - val_acc: 0.7140\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.2412 - acc: 0.9050 - val_loss: 0.6016 - val_acc: 0.7310\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.2127 - acc: 0.9160 - val_loss: 0.8034 - val_acc: 0.7000\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.1900 - acc: 0.9300 - val_loss: 0.7055 - val_acc: 0.7170\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 46s 455ms/step - loss: 0.1716 - acc: 0.9345 - val_loss: 0.6932 - val_acc: 0.7280\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 0.1521 - acc: 0.9500 - val_loss: 0.7441 - val_acc: 0.7230\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.1349 - acc: 0.9560 - val_loss: 0.7433 - val_acc: 0.7150\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 45s 454ms/step - loss: 0.1104 - acc: 0.9640 - val_loss: 0.8706 - val_acc: 0.7250\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.1028 - acc: 0.9680 - val_loss: 0.8598 - val_acc: 0.7140\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0790 - acc: 0.9790 - val_loss: 0.9063 - val_acc: 0.7200\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.0704 - acc: 0.9810 - val_loss: 0.8483 - val_acc: 0.7340\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0562 - acc: 0.9845 - val_loss: 0.9237 - val_acc: 0.7190\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.0511 - acc: 0.9860 - val_loss: 1.1270 - val_acc: 0.6900\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0383 - acc: 0.9930 - val_loss: 1.2588 - val_acc: 0.7110\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0391 - acc: 0.9890 - val_loss: 1.1697 - val_acc: 0.7050\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0410 - acc: 0.9880 - val_loss: 1.0902 - val_acc: 0.7340\n"
     ]
    }
   ],
   "source": [
    "#배치 제너레이터 사용해 모델 훈련\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                             steps_per_epoch=100,\n",
    "                             epochs=30,\n",
    "                             validation_data = validation_generator,\n",
    "                             validation_steps=50)\n",
    "\n",
    "#모델 저장\n",
    "model.save('cats_and_dogs_small_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련의 정확도와 손실 그래프 그리기\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#과대적합의 특성 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.5 데이터 증식 사용하기\n",
    "\n",
    "기존 훈련 샘플로부터 더 많은 훈련 데이터 생성하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ImageDataCenerator 사용해 데이터 증식 설정하기\n",
    "\n",
    "datagen = ImageDataGeneratorGeneratorDataGenerator(rotation_range=20, #랜덤하게 사진 회전시킬 각도 범위(0~180)\n",
    "                            width_shift_range=0.1, #사진을 수평/수직으로 랜덤하게 평행 이동시킬 범위(전체 높이,너비에 대한 비율)\n",
    "                            height_shift_range=0.1,\n",
    "                            shear_range=0.1, #랜덤하게 전단 변환 적용할 각도 범위\n",
    "                            zoom_range=0.1, #랜덤하게 사진 확대할 범위\n",
    "                            horizontal_flip=True, #랜덤하게 이미지 수평 뒤집기\n",
    "                            fill_mode='nearest') #회전, 가로/세로 이동으로 인해 새롭게 생성해야 할 픽셀 채울 전략"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#드롭아웃을 포함한 새로운 컨브넷 정의하기\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b568779a8ea3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m                                                        class_mode='binary')\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m history = model.fit.generator(train_generator,\n\u001b[0m\u001b[0;32m     24\u001b[0m                              \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                              \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'generator'"
     ]
    }
   ],
   "source": [
    "#데이터 증식 제너레이터 사용해 컨브넷 훈련하기\n",
    "\n",
    "train_datagen = ImageDataGeneratorGeneratorGeneratorGenerator(rescale = 1./255,\n",
    "                                  rotation_range=40,\n",
    "                                  width_shift_range=0.2,\n",
    "                                  height_shift_range=0.2,\n",
    "                                  shear_range=0.2,\n",
    "                                  zoom_range=0.2,\n",
    "                                  horizontal_flip=True,)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) #검증데이터는 증식되면 안됨\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                   target_size=(150,150),\n",
    "                                                   batch_size=32,\n",
    "                                                   class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir,\n",
    "                                                       target_size=(150,150),\n",
    "                                                       batch_size=32,\n",
    "                                                       class_mode='binary')\n",
    "\n",
    "history = model.fit.generator(train_generator,\n",
    "                             steps_per_epoch = 100,\n",
    "                             epochs=100,\n",
    "                             validation_data=validation_generator,\n",
    "                             validation_steps=50)\n",
    "\n",
    "#모델 저장\n",
    "model.save('cats_and_dogs_small_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 사전 훈련된 컨브넷 사용하기\n",
    "\n",
    "대규모 이미지 분류 문제 위해 대량의 데이터셋에서 미리 훈련되어 저장된 네트워크\n",
    "\n",
    "사전 훈련된 네트워크 사용하는 방법\n",
    "1. 특성 추출\n",
    "2. 미세 조정\n",
    "\n",
    "\n",
    "#### 5.3.1 특성 추출\n",
    "\n",
    "사전에 학습된 네트워크 표현 사용해 새로운 샘플에서 흥미로운 특성 뽑아내는 것   \n",
    "특성 사용해 새로운 분류기 처음부터 훈련\n",
    "\n",
    "연속된 합성곱, 풀링 층으로 시작해 완전 연결 분류기로 끝남\n",
    "\n",
    "합성곱 층만 재사용, 완전 연결 분류기는 일반적으로 재사용 X\n",
    "\n",
    "특성 합성곱 층에서 추출한 표현의 일반성(과 재사용성) 수준은 모델에 있는 층의 깊이에 달려 있음   \n",
    "모델의 하위 층은 (에지, 색, 질감 등) 지역적, 매우 일반적 특성 맵 추출함   \n",
    "상위 층은 ('강아지 눈', '고양이 귀' 등) 추상적인 개념 추출   \n",
    "-> 모델의 하위 층 몇 개만 특성 추출에 사용하는 것이 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 16s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#VGG16 합성곱 기반 층 만들기\n",
    "\n",
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet', #모델 초기화할 가중치 체크포인트 지정\n",
    "                 include_top=False, #네트워크 최상위 완전 연결 분류기 포함 여부 지정(기본값은 포함)\n",
    "                 input_shape=(150,150,3)) #네트워크에 주입할 이미지 텐서의 크기, 선택 사항(지정 안하면 어떤 크기의 입력도 처리 가능)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**데이터 증식 사용하지 않는 빠른 특성 추출**   \n",
    "\n",
    "빠르고 비용 적게 듬, 데이터 증식 사용 불가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사전 훈련된 합성곱 기반 층 사용한 특성 추출하기\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "base_dir = './datasets/cats_and_dogs_small'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale = 1./255)\n",
    "batch_size = 20\n",
    "\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape =(sample_count, 4,4,512))\n",
    "    labels=np.zeros(shape=(sample_count))\n",
    "    generator=datagen.flow_from_directory(directory,\n",
    "                                         target_size=(150,150),\n",
    "                                         batch_size=batch_size,\n",
    "                                         class_mode='binary')\n",
    "    i=0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i*batch_size : (i+1) * batch_size] = features_batch\n",
    "        labels[i*batch_size : (i+1) * batch_size] = labels_batch\n",
    "        i+=1\n",
    "        if i * batch_size >=sample_count:\n",
    "            break\n",
    "            #제너레이터는 루프 안에서 무한하게 데이터 만들어 내서 모든 이미지 한 번씩 처리 후에는 중지함\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = extract_features(train_dir, 2000)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 1000)\n",
    "test_features, test_labels = extract_features(test_dir, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
