{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 신경망 시작하기\n",
    "\n",
    "## 3.1 신경망의 구조\n",
    "    \n",
    "신경망 훈련의 요소들    \n",
    "+ 네트워크(또는 모델)을 구성하는 **층**   \n",
    "+ **입력 데이터**와 그에 상응하는 **타깃**   \n",
    "+ 학습에 사용할 피드백 신호 정의 하는 **손실 함수**   \n",
    "+ 학습 진행 방식 결정하는 **옵티마이저**\n",
    "\n",
    "#### 3.1.1 층: 딥러닝의 구성 단위\n",
    "\n",
    "층: 하나 이상의 텐서를 입력으로 받아 하나 이상의 텐서 출력하는 데이터 처리 모듈\n",
    "\n",
    "    대부분 가중치라는 층의 상태를 가짐   \n",
    "    가중치: 확률적 경사 하강법에 의해 학습됨   \n",
    "            네트워크가 학습한 지식이 담겨 있음   \n",
    "            \n",
    "층마다 적절한 텐서 포맷과 데이터 처리 방식이 다름   \n",
    "2D 텐서가 저장된 간단한 벡터 데이터는 완전 연결층이나 밀집 층이라 불리는 밀집 연결 층에 의해 처리되는 경우가 많음   \n",
    "3D 텐서가 저장된 시퀀스 데이터는 LSTM 같은 순환 층에 의해 처리됨   \n",
    "4D 텐서로 저장된 이미지 데이터는 일반적으로 2D 합성곱 층에 의해 처리됨\n",
    "\n",
    "층 = 딥러닝의 레고 블록\n",
    "\n",
    "케라스에서 호환 가능한 층들 엮어 데이터 변환 파이프라인을 구성해 딥러닝 모델을 만듬\n",
    "\n",
    "층 호환성: 각 층이 특정 크기의 입력 텐서만 받고 특정 크기의 출력 텐서를 반환한다는 사실   \n",
    "케라스에서는 모델에 추가된 층을 자동으로 상위 층의 크기에 맞춰 줘서 호환성 걱정 안해도 됨\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32,input_shape=(784,)))\n",
    "model.add(layers.Dense(10)) #두번째 층에서는 input_shape 매개변수를 지정하지 않고 앞의 층의 출력 크기를 입력 크기로 자동 채택함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 모델: 층의 네트워크\n",
    "\n",
    "딥러닝 모델: 층으로 만든 비순환 유향 그래프\n",
    "\n",
    "자주 등장하는 네트워크 구조\n",
    "- 가지가 2개인 네트워크\n",
    "- 출력이 여러 개인 네트워크\n",
    "- 인셉션 블록\n",
    "\n",
    "네트워크 구조는 가설 공간을 정의함   \n",
    "네트워크 구조를 선택해 가능성 있는 공간(가설 공간)을 입력 -> 출력 데이터로 매핑하는 일련의 특정 텐서 연산으로 제한함   \n",
    "텐서 연산에 포함된 가중치 텐서의 좋은 값을 찾아야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 손실 함수와 옵티마이저: 학습 과정을 조절하는 열쇠\n",
    "\n",
    "손실 함수(목적 함수): 훈련하는 동안 최소화될 값, 주어진 문제에 대한 성공 지표가 됨\n",
    "\n",
    "옵티마이저: 손실 함수 기반으로 네트워크가 어떻게 업데이트될지 결정함, 특정 종류의 확률적 경사 하강법(SGD)을 구현함\n",
    "\n",
    "여러 개의 출력을 내는 신경망은 여러 개의 손실 함수를 가질 수 있음   \n",
    "경사 하강법 과정은 하나의 스칼라 손실 값을 기준으로 함   \n",
    "-> 손실이 여러 개인 네트워크에서는 모든 손실이 (평균을 내서) 하나의 스칼라 양으로 합쳐짐\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "**분류, 회귀, 시퀀스 예측과 같은 일반적인 문제에서의 올바른 손실 함수를 선택하는 지침**\n",
    "- 이진 크로스엔트로피: 2개의 클래스가 있는 분류 문제\n",
    "- 범주형 크로스엔트로피: 여러 개의 클래스가 있는 분류 문제\n",
    "- 평균 제곱 오차: 회귀 문제\n",
    "- CTC: 시퀀스 학습 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 케라스 소개\n",
    "\n",
    "케라스: 거의 모든 종류의 딥러닝 모델을 간편하게 만들고 훈련시킬 수 있는 파이썬을 위한 딥러닝 프레임워크\n",
    "\n",
    "<br/>\n",
    "\n",
    "**특징**\n",
    "- 동일한 코드로 CPU와 GPU에서 실행 가능\n",
    "- 사용하기 쉬운 API를 가지고 있어서 딥러닝 모델의 프로토타입을 빠르게 만들 수 있음\n",
    "- (컴퓨터 비전을 위한) 합성곱 신경망, (시퀀스 처리를 위한) 순환 신경망을 지원, 둘을 자유롭게 조합해 사용 가능\n",
    "- 다중 입력, 다중 출력 모델, 층의 공유, 모델 공유 등 어떤 네트워크 구조도 만들 수 있음 = 어떤 딥러닝 모델에도 적합함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 케라스, 텐서플로, 씨아노, CNTK\n",
    "\n",
    "케라스 특징\n",
    "- 케라스는 텐서 조작이나 미분 같은 저수준의 연산을 다루지 않음  \n",
    "- 대신 케라스의 벡엔드 엔진에서 제공하는 최적화되고 특화된 텐서 라이브러리를 사용함\n",
    "- 모듈 구조로 구성되어 있음(텐서 라이브러리에 국한해서 구현되지 않음)\n",
    "- 여러 백엔드 엔진이 케라스와 매끄럽게 연동됨(텐서플로, 씨아노, 마이크로 소프트 코그니티브 툴킷)\n",
    "\n",
    "텐서플로(구글), CNTK(마이크로소프트), 씨아노(몬트리올 대학 연구소)는 딥러닝을 위한 주요 플랫폼 중 하나   \n",
    "케라스로 작성한 코드는 개발 중간에 언제든지 백엔드를 바꿀 수 있음\n",
    "\n",
    "CPU에서 실행될 때 텐서플로는 저수준 텐서 연산 라이브러리 이용   \n",
    "GPU에서 실행될 때 NVIDIA CUDA 심층 신경망 라이브러리(cuDNN)라고 불리는 고도로 최적화된 딥러닝 연산 라이브러리 이용\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 케라스를 사용한 개발: 빠르게 둘러보기\n",
    "\n",
    "케라스 작업 흐름\n",
    "1. 입력 텐서와 타깃 텐서로 이루어진 훈련 데이터 정의\n",
    "2. 입력과 타깃을 매핑하는 층으로 이루어진 네트워크(모델) 정의\n",
    "\n",
    "       정의 방법: Sequential 클래스 - 가장 자주 사용하는 구조인 층을 순서대로 쌓아 올린 네트워크\n",
    "                   함수형 API - 완전히 임의의 구조를 만들 수 있는 비순환 유향 그래프를 만듬\n",
    "3. 손실 함수, 옵티마이저, 모니터링하기 위한 측정 지표를 선택해 학습 과정 설정\n",
    "       \n",
    "       컴파일 단계에서 학습 과정이 설정되고 모델이 사용할 옵티마이저, 손실 함수, 측정 지표를 지정함\n",
    "4. 훈련 데이터에 대해 모델의 fit() 메서드 반복적 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequential 클래스를 사용해 정의한 2개의 층으로 된 모델\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32,input_shape=(784,)))\n",
    "model.add(layers.Dense(10)) #두번째 층에서는 input_shape 매개변수를 지정하지 않고 앞의 층의 출력 크기를 입력 크기로 자동 채택함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#함수형 API를 사용해 만든 모델\n",
    "\n",
    "input_tensor = layers.Input(shape=(784,))\n",
    "x=layers.Dense(32,activation='relu')(input_tensor)\n",
    "output_tensor=layers.Dense(10,activation='softmax')(x)\n",
    "\n",
    "model=models.Model(inputs=input_tensor,outputs=output_tensor)\n",
    "#모델이 처리할 데이터 텐서를 만들고 함수처럼 이 텐서에 층을 적용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bc5e32cfb647>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#입력 데이터의 넘파이 배열과 상응하는 타깃 데이터를 모델의 fit() 메서드에 전달해 학습 과정이 이뤄짐\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'target_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "#하나의 손실 함수를 사용하는 가장 흔한 경우의 예시\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "             loss='mse',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "#입력 데이터의 넘파이 배열과 상응하는 타깃 데이터를 모델의 fit() 메서드에 전달해 학습 과정이 이뤄짐\n",
    "model.fit(input_tensor,target_tensor,batch_size=128,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 딥러닝 컴퓨터 셋팅\n",
    "\n",
    "최신 NVIDIA GPU에서 딥러닝 코드를 실행하는 것을 권장함   \n",
    "유닉스 운영체제 사용이 좋음(윈도우는 권장 X)\n",
    "\n",
    "## 3.4 영화 리뷰 분류: 이진 분류 예제\n",
    "\n",
    "리뷰 텍스트를 기반으로 영화 리뷰를 긍정과 부정으로 분류하는 방법\n",
    "\n",
    "#### 3.4.1 IMDB 데이터셋\n",
    "\n",
    "IMDB 데이터셋: 양극단의 리뷰 5만개로 이루어져 있음\n",
    "\n",
    "               전처리되어 있어 각 리뷰(단어 시퀀스)가 숫자 시퀀스로 변화되더 있음\n",
    "               각 숫자는 사전에 있는 고유한 단어를 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터셋 로드\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "# num_words=10000 : 훈련 데이터에서 가장 자주 나타나는 단어 10000개만 사용하겠다는 의미\n",
    "\n",
    "#train_labels, test_labels는 부정(0)과 긍정(1)의 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 데이터 준비\n",
    "\n",
    "신경망에 숫자 리스트를 주입할 수 없음  \n",
    "\n",
    "리스트를 텐서로 바꾸는 두 가지 방법   \n",
    "- 같은 길이가 되도록 패딩을 추가하고 (samples, sequence_length) 크기의 정수 텐서로 변환   \n",
    "  이 정수 텐서를 다룰 수 있는 층을 신경망의 첫 번째 층으로 사용\n",
    "- 리스트를 원-핫 인코딩해 0과 1의 벡터로 변환   \n",
    "  ex) 시퀀스 [3,5]를 인덱스 3,5의 위치는 1이고 나머지는 모두 0인 10000차원 벡터로 각각 변환함   \n",
    "     그 후 부동 소수 벡터 데이터 다룰 수 있는 Dense 층을 신경망 첫 번째 층으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터를 원-핫 벡터로 만들기\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension)) #크기가 (len(sequences), dimension)이고 모든 원소 0인 행렬 만듬\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i,sequence]=1. #results[i]에서 특정 인덱스 위치를 1로 만듬\n",
    "    return results\n",
    "\n",
    "#훈련, 테스트 데이터를 벡터로 변환\n",
    "x_train=vectorize_sequences(train_data)\n",
    "x_test=vectorize_sequences(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블을 벡터로 바꿈\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3 신경망 모델 만들기\n",
    "\n",
    "입력 데이터가 벡터, 레이블은 스칼라(1 또는 0)   \n",
    "이런 문제에 잘 작동하는 네트워크 종류는 relu 활성화 함수를 사용한 완전 연결 층을 그냥 쌓은 것(Dense(16,activation='relu')   \n",
    "Dense층에 전달한 매개변수 (!6)은 은닉 유닛의 개수   \n",
    "하나의 은닉 유닛: 층이 나타내는 표현 공간에서 하나의 차원이 됨\n",
    "\n",
    "16개의 은닉 유닛이 있다는 것 = 가중치 행렬 w의 크기가 (input_dimension, 16)이라는 뜻   \n",
    "입력 데이터와 w를 점곱 -> 입력 데이터가 16차원으로 표현된 공간으로 투영됨\n",
    "\n",
    "은닉 유닛 늘리면 신경망이 더 복잡한 표현 학습 가능, 계산 비용 커지고 원치 않는 패턴 학습할 수도 있음\n",
    "\n",
    "Dense 층 쌓을 때 중요한 구조상의 결정\n",
    "- 얼마나 많은 층을 사용할지\n",
    "- 각 층에 얼마나 많은 은닉 유닛을 둘 것인지\n",
    "\n",
    "선택한 구조\n",
    "- 16개의 은닉 유닛을 가진 2개의 은닉 층\n",
    "- 현재 리뷰의 감정을 스칼라 값의 예측으로 출력하는 세 번째 층\n",
    "        \n",
    "        중간에 있는 은닉 층: 활성화 함수로 relu(음수를 0으로 만듬) 사용   \n",
    "        마지막 층은 확률 출력 위해 시그모이드 활성화 함수(임의의 값을 [0,1] 사이로 압축) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 정의\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model=models.Sequential()\n",
    "model.add(layers.Dense(16,activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16,activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이진 분류 문제, 신경망의 출력이 확률이라 binary_crossentropy 손실이 적합함(mean_squared_error도 사용 가능)   \n",
    "확률 출력하는 모델 사용할 때는 크로스엔트로피(확률 분포 간의 차이 측정)가 최선의 선택임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 컴파일\n",
    "#rmsprop 옵티마이저, binary_crossentropy 손실 함수로 모델 설정\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#옵티마이저의 매개변수 바꿀 때\n",
    "#옵티마이저 파이썬 클래스 사용해 객체를 직접 만들어서 optimizer 매개변수에 전달\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#자신만의 손실 함수, 측정 함수를 전달할 때\n",
    "#loss와 metrics 매개변수에 함수 객체 전달\n",
    "\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "             loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 컴파일\n",
    "#rmsprop 옵티마이저, binary_crossentropy 손실 함수로 모델 설정\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.4 훈련 검증\n",
    "\n",
    "모델의 정확도 측정을 위해서는 원본 훈련 데이터에서 10000개의 샘플을 떼어 검증 세트 만들어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#검증 세트 준비하기\n",
    "\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 훈련하기\n",
    "#모델을 512개의 샘플씩 미니 배치를 만들어 20번의 에포크 동안 훈련\n",
    "#동시에 따로 떼어 놓은 10000개의 샘플에서 손실과 정확도를 측정\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "#model.fit() 메서드는 History 객체를 반환함(훈련하는 동안 발생한 모든 정보를 담고 있는 딕셔너리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련과 검증 손실 그리기\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict=history.history\n",
    "loss=history_dict['loss']\n",
    "val_loss=history_dict['val_loss']\n",
    "\n",
    "epochs = range(1,len(loss)+1)\n",
    "\n",
    "plt.plot(epochs,loss,'bo',label='Training loss') #bo는 파란색 점\n",
    "plt.plot(epochs,val_loss,'b',label='Validation loss') #b는 파란색 실선\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#훈련 손실이 에포크마다 감소하고 훈련 정확도는 에포크마다 증가함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련과 검증 정확도\n",
    "\n",
    "plt.clf() #그래프 초기화\n",
    "acc=history_dict['acc']\n",
    "val_acc=history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs,acc,'bo',label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#과적합됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#과적합을 막기 위해 모델을 처음부터 다시 4번의 에포크 동안만 훈련하기\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16,activation='relu',input_shape=(10000,)))\n",
    "model.add(layers.Dense(16,activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train,epochs=4,batch_size=512)\n",
    "results=model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.5 훈련된 모델로 새로운 데이터에 대해 예측하기\n",
    "\n",
    "predict 메서드를 사용해 어떤 리뷰가 긍정일 확률을 예측할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 뉴스 기사 분류: 다중 분류 문제\n",
    "\n",
    "로이터 뉴스를 46개의 상호 배타적인 토픽으로 분류하는 신경망 만들기   \n",
    "다중 분류의 예   \n",
    "각 데이터 포인트가 정확히 하나의 범주로 분류돼서 **단일 레이블 다중 분류** 문제\n",
    "\n",
    "#### 3.5.1 로이터 데이터셋\n",
    "\n",
    "1986년에 로이터에서 공개한 짧은 뉴스 기사와 토픽의 집합   \n",
    "46개의 토픽이 있고 각 토픽은 훈련 세트에 최소 10개의 샘플을 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터셋 로드\n",
    "\n",
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels)=reuters.load_data(num_words=10000) #가장 자주 등장하는 단어 10000개로 제한\n",
    "#각 샘플은 정수 리스트(단어 인덱스)\n",
    "\n",
    "#로이터 데이터셋을 텍스트로 디코딩\n",
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index=dict([(value,key) for (key,value) in word_index.items()])\n",
    "decoded_newswire=' '.join([reverse_word_index.get(i - 3,'?') for i in train_data[0]])\n",
    "#0,1,2는 패딩, 문서 시작, 사전에 없음을 위한 인덱스라서 3을 뺌\n",
    "\n",
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 인코딩하기\n",
    "#데이터를 벡터로 변환하기\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "#훈련, 테스트 데이터 벡터 변환\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블을 벡터로 바꾸는 방법\n",
    "1. 레이블의 리스트를 정수 텐서로 변환하는 것\n",
    "2. 원-핫 인코딩을 사용하는 것\n",
    "\n",
    "원-핫 인코딩이 범주형 데이터에 널리 사용돼서 **범주형 인코딩**이라고도 부름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.3 모델 구성\n",
    "\n",
    "앞의 문제와 짧은 텍스트를 분류하는 것은 같음   \n",
    "출력 클래스의 개수가 2-> 46개로 늘어남(출력 공간의 차원이 커짐)\n",
    "\n",
    "이전처럼 Dense 층 쌓으면 한 층이 일부 정보 누락 시 다음 층에서 복원할 수 없음   \n",
    "-> 각 층은 잠재적으로 정보의 병목이 될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주목해야 할 점\n",
    "- 마지막 Dense 층의 크기가 46(각 입력 샘플에 대해 46차원의 벡터를 출력)   \n",
    "  벡터의 각 원소(차원)은 다른 출력 클래스가 인코딩 된 것\n",
    "- 마지막 층에 softmax 활성화 함수가 사용됨   \n",
    "  각 입력 샘플마다 46개의 출력 클래스에 대한 확률 분포를 출력함(46개의 값 모두 더하면 1)\n",
    "  \n",
    "이런 문제에 사용할 최선의 손실 함수는 categorical_crossentropy   \n",
    "두 확률 분포 사이의 거리를 측정함(여기에서는 네트워크가 출력한 확률 분포와 진짜 레이블의 분포 사이의 거리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 컴파일\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.4 훈련 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련 데이터에서 1000개의 샘플 따로 떼서 검증 세트로 사용\n",
    "#검증 세트 준비하기\n",
    "\n",
    "x_val=x_train[:1000]\n",
    "partial_x_train=x_train[1000:]\n",
    "y_val=one_hot_train_labels[:1000]\n",
    "partial_y_train=one_hot_train_labels[1000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 훈련하기(20번의 에포크로)\n",
    "\n",
    "history=model.fit(partial_x_train,\n",
    "                 partial_y_train,\n",
    "                 epochs=20,\n",
    "                 batch_size=512,\n",
    "                 validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련과 검증 손실 그리기\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(1,len(loss)+1)\n",
    "\n",
    "plt.plot(epochs,loss,'bo',label='Training loss')\n",
    "plt.plot(epochs,val_loss,'b',label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련과 검증 정확도 그리기\n",
    "\n",
    "#plt.clf() #그래프 초기화\n",
    "\n",
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델을 처음부터 다시 훈련하기\n",
    "\n",
    "model=models.Sequential()\n",
    "model.add(layers.Dense(64,activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64,activation='relu'))\n",
    "model.add(layers.Dense(46,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "         partial_y_train,\n",
    "         epochs=9,\n",
    "         batch_size=512,\n",
    "         validation_data=(x_val,y_val))\n",
    "\n",
    "results=model.evaluate(x_test,one_hot_test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.5 새로운 데이터에 대해 예측하기\n",
    "\n",
    "모든 객체의 predict 메서드는 46개의 토픽에 대한 확률 분포를 반환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(x_test)\n",
    "#predictions의 각 항목은 길이 46인 벡터, 벡터의 원소합은 1\n",
    "#가장 큰 값이 예측 클래스가 됨(가장 확률 높은 클래스)\n",
    "\n",
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.6 레이블과 손실을 다루는 다른 방법\n",
    "\n",
    "정수 텐서로 변환하는 것\n",
    "\n",
    "```\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "```\n",
    "이 방식을 사용하려면 손실 함수만 바꾸면 됨\n",
    "\n",
    "- categorical_crossentropy : 레이블이 범주형 인코딩되어 있을 것이라고 기대\n",
    "- sparse_categorical_crossentropy ; 정수 레이블을 사용할 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.7 충분히 큰 중간층을 두어야 하는 이유\n",
    "\n",
    "마지막 출력이 46이라 중간층의 히든 유닛은 46개보다 많이 적어서는 안됨   \n",
    "-> 병목이 나타남\n",
    "\n",
    "많은 정보를 중간층의 저차원 표현 공간으로 압축하려고 하기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 주택 가격 예측: 회귀 문제\n",
    "\n",
    "개별적인 레이블 대신 연속적인 값을 예측하는 회귀 문제   \n",
    "ex) 기상 데이터가 주어졌을 때 내일 기온을 예측 / 소프트웨어 명세가 주어졌을 때 소프트웨어 프로젝트가 완료될 시간 예측\n",
    "\n",
    "※회귀와 로지스틱 회귀 알고리즘(분류 알고리즘)은 다름\n",
    "\n",
    "#### 3.6.1 보스턴 주택 가격 데이터셋\n",
    "\n",
    "1970년 중반 보스턴 외곽 지역의 범죄율, 지방세율 등의 데이터가 주어졌을 때 주택 가격의 중간 값 예측\n",
    "\n",
    "데이터 포인트가 506개로 404개가 훈련 샘플, 102개가 테스트 샘플   \n",
    "입력 데이터의 각 특성은 스케일이 서로 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터셋 로드\n",
    "\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "\n",
    "#타깃은 주택의 중간 가격으로 천 달러 단위임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2 데이터 준비\n",
    "\n",
    "상이한 스케일을 가진 데이터를 다룰 때 특성별로 정규화를 하는 것이 대표적 방법   \n",
    "입력 데이터에 있는 각 특성(입력 데이터 행렬의 열)에 대해 특성의 평균을 빼고 표준편차로 나눔   \n",
    "-> 특성의 중앙이 0 근처에 맞춰지고 표준 편차가 1이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 정규화하기\n",
    "\n",
    "mean=train_data.mean(axis=0)\n",
    "train_data-=mean\n",
    "std=train_data.std(axis=0)\n",
    "train_data/=std\n",
    "\n",
    "test_data-=mean\n",
    "test_data/=std\n",
    "\n",
    "#테스트 데이터 정규화할 때 사용한 값이 훈련 데이터에서 계산한 값임!!\n",
    "#머신 러닝 작업 과정에서 테스트 데이터에서 계산한 그 어떤 값도 사용해서는 안됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.3 모델 구성\n",
    "\n",
    "샘플 개수가 적어서 64개의 유닛 가진 2개의 은닉 층으로 작은 네트워크 구성해 사용   \n",
    "훈련 데이터 개수가 적을수록 과대적합이 더 쉽게 일어나서 작은 모델 사용하는 것이 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 정의하기\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model(): #동일 모델을 여러 번 생성할 거라서 함수 만듬\n",
    "    model=models.Sequential()\n",
    "    model.add(layers.Dense(64,activation='relu',\n",
    "                          input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64,activation='relu'))\n",
    "    model.add(layers.Dense(1)) \n",
    "    #하나의 유닛 가지고 활성화 함수가 없음(선형 층) -> 전형적인 스칼라 회귀(하나의 연속적 값 예측하는 회귀)를 위한 구성\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    #mse 솔실 함수: 평균 제곱 오차의 약어, 예측과 타깃 사이 거리의 제곱\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.4 K-겹 검증을 사용한 훈련 검증\n",
    "\n",
    "데이터를 훈련/검증 세트로 나눔(매개변수들 조정하면서 모델 평가 위해)   \n",
    "데이터 포인트가 많지 않아 검증 세트도 매우 작아짐(약 100개)   \n",
    "-> 검증 세트와 훈련 세트로 어떤 데이터 포인트가 선택됐는지에 따라 검증 점수가 크게 달라짐(신뢰 x)\n",
    "\n",
    "이런 상황에서 **K-겹 교차 검증**을 사용하는 것이 좋음\n",
    "\n",
    "데이터를 K개의 분할(폴드)로 나누고 K개의 모델을 각각 만들어 k-1개의 분할에서 훈련하고 나머지 분할에서 평가하는 방법   \n",
    "모델 검증 점수 = K개의 검증 점수 평균   \n",
    "일반적으로 K=4 또는 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-겹 검증하기\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "k=4\n",
    "\n",
    "num_val_samples = len(train_data) # k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print('처리중인 폴드 #', i)\n",
    "    # 검증 데이터 준비: k번째 분할\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # 훈련 데이터 준비: 다른 분할 전체\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    \n",
    "    model = build_model() # 케라스 모델 구성(컴파일 포함)\n",
    "    # 모델 훈련(verbose=0 이므로 훈련 과정이 출력되지 않음)\n",
    "    model.fit(partial_train_data, partial_train_targets,\n",
    "              epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    # 검증 세트로 모델 평가\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#각 폴드에서 검증 점수를 로그에 저장\n",
    "\n",
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "    print('처리중인 폴드 #', i)\n",
    "    # 검증 데이터 준비: k번째 분할\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # 훈련 데이터 준비: 다른 분할 전체\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # 케라스 모델 구성(컴파일 포함)\n",
    "    model = build_model()\n",
    "    # 모델 훈련(verbose=0 이므로 훈련 과정이 출력되지 않음)\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    mae_history = history.history['val_mean_absolute_error']\n",
    "    all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모든 폴드에 대해 에포크의 MAE 점수 평균 계산\n",
    "#K-겹 검증 점수 평균 기록하기\n",
    "\n",
    "average_mae_history=[np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "\n",
    "#검증 점수 그래프\n",
    "\n",
    "\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#곡선의 다른 부분들과 스케일이 많은 다른 첫 10개의 데이터 포인터 제외시킴\n",
    "#부드러운 곡선 얻기 위해 각 포인트를 이전 포인트의 지수 이동 평균으로 대체\n",
    "\n",
    "#처음 10개 데이터 포인트 제외한 검증 점수 그리기\n",
    "def smooth_curve(points, factor=0.9):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points\n",
    "\n",
    "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n",
    "\n",
    "#검증 MAE가 80번째 에포크 이후에 줄어드는 것이 멈춤(이후부턴 과대적합 시작)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#최조 모델 훈련하기\n",
    "\n",
    "model = build_model() #새롭게 컴파일된 모델 얻음\n",
    "model.fit(train_data, train_targets, epochs=80, batch_size=16, verbose=0)\n",
    "# 전체 데이터로 훈련시킴\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
